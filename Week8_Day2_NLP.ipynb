{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0jujWZpXHBJKvcOp3Svpf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arielwendichansky/DI_Bootcamp/blob/master/Week8_Day2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYVENsbmsqZo"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize words"
      ],
      "metadata": {
        "id": "PVKUonWl2gVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz3jnsEoxCz8",
        "outputId": "59abad88-1a77-4667-98cc-7ce54aaa8cab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "doc = 'We are learning NLP. I am excited about it.'\n",
        "tokens = word_tokenize(doc)\n",
        "print(tokens)\n",
        "\n",
        "span = tokens[1:7]\n",
        "print(span)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKae2PQ0xE9H",
        "outputId": "143f30ea-8168-4d00-ec27-8f5c8459ec53"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'are', 'learning', 'NLP', '.', 'I', 'am', 'excited', 'about', 'it', '.']\n",
            "['are', 'learning', 'NLP', '.', 'I', 'am']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "span can be a chunk or a group of tokens. The definition of a span varies depending on the context and the specific task at hand. It could be a sentence, or a group of tokens that collectively form a named entity, such as “United States of America.\n",
        "\n",
        "A token is a unit of language. Can be anything with meaning: a word, number, or punctuation.\n"
      ],
      "metadata": {
        "id": "sKW2fSVFygh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"Why, sometimes I`ve believed as many as 6 impossible things before breakfast?\"\n",
        "tokens = word_tokenize(doc)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZHktAGLykAi",
        "outputId": "085e48a8-b222-470d-94c9-c3f93476d83c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Why', ',', 'sometimes', 'I', '`', 've', 'believed', 'as', 'many', 'as', '6', 'impossible', 'things', 'before', 'breakfast', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "doc = \"Why, sometimes I've believed as many as 6 impossible things before breakfast? Focus on things that you have come to believe are impossible or improbable, and start believing in them.\"\n",
        "sentences = sent_tokenize(doc)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcE-9yy0zo2p",
        "outputId": "0255f4ce-e914-4e6d-bfa0-211497e84194"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Why, sometimes I've believed as many as 6 impossible things before breakfast?\", 'Focus on things that you have come to believe are impossible or improbable, and start believing in them.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords"
      ],
      "metadata": {
        "id": "7o1_Spwg2cn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOBAtz2c1pU4",
        "outputId": "fdcfcbcc-a409-49f7-e1cb-caf2a2b0554b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXHtAzMH1zA_",
        "outputId": "e2ac009b-0032-4445-9c3d-0b1592b019ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = '''Linguistics and Natural Language Processing (NLP) are closely linked. \\n Linguistics is the scientific study of language, encompassing its structure, meaning, and context. \\n It provides foundational knowledge about language syntax, semantics, pragmatics, and phonetics. \\n NLP applies this linguistic knowledge in computational algorithms to enable computers to understand, interpret, and generate human language. By leveraging linguistic principles, NLP seeks to bridge the gap between human communication and computer understanding, enabling tasks like translation, sentiment analysis, and voice recognition.'''\n",
        "tokens = word_tokenize(doc)\n",
        "\n",
        "cleaned_doc = [ word for word in tokens if word not in stopwords.words('english') ]\n",
        "print(cleaned_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0jajQ8i2kTq",
        "outputId": "b8f37593-4e27-4889-ee10-dced59ade87a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Linguistics', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'closely', 'linked', '.', 'Linguistics', 'scientific', 'study', 'language', ',', 'encompassing', 'structure', ',', 'meaning', ',', 'context', '.', 'It', 'provides', 'foundational', 'knowledge', 'language', 'syntax', ',', 'semantics', ',', 'pragmatics', ',', 'phonetics', '.', 'NLP', 'applies', 'linguistic', 'knowledge', 'computational', 'algorithms', 'enable', 'computers', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '.', 'By', 'leveraging', 'linguistic', 'principles', ',', 'NLP', 'seeks', 'bridge', 'gap', 'human', 'communication', 'computer', 'understanding', ',', 'enabling', 'tasks', 'like', 'translation', ',', 'sentiment', 'analysis', ',', 'voice', 'recognition', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "Stemming is the technique that reduces words to their base or root form. For instance, “running” becomes “run”.\n",
        "\n"
      ],
      "metadata": {
        "id": "iMaWyus13Bjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2URXYuKF3FPt",
        "outputId": "87965dc2-7ef3-441c-9821-af4eb965b166"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['charl', 'lutwidg', 'dodgson', ',', 'better', 'known', 'by', 'hi', 'pen', 'name', 'lewi', 'carrol', ',', 'wa', 'an', 'english', 'author', ',', 'poet', ',', 'mathematician', 'and', 'photograph', '.', 'hi', 'most', 'notabl', 'work', 'are', 'alic', \"'s\", 'adventur', 'in', 'wonderland', '(', '1865', ')', 'and', 'it', 'sequel', 'through', 'the', 'looking-glass', '(', '1871', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "\n",
        "It considers the context and converts the word to its meaningful base form. For example, “is” is changed to “be”.\n"
      ],
      "metadata": {
        "id": "IiJggq_D4Hnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "fVpzfpCq4NkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "#load spaCy's English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = 'We are learning NLP. I am excited about it.'\n",
        "\n",
        "#process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "filtered_doc = [word for word in doc if word not in stopwords.words('english')]\n",
        "\n",
        "lemmatized = [token.lemma_ for token in filtered_doc]\n",
        "print('Lemmatized: ', lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDqM6g-o4Rxo",
        "outputId": "6881b7e6-47e8-4bf0-af68-cb037f6165e6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized:  ['we', 'be', 'learn', 'NLP', '.', 'I', 'be', 'excited', 'about', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming Vs Lemmatization"
      ],
      "metadata": {
        "id": "MsN5xRLS4tBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"Charles Lutwidge Dodgson, better known by his pen name Lewis Carroll, was an English author, poet, mathematician and photographer. His most notable works are Alice's Adventures in Wonderland (1865) and its sequel Through the Looking-Glass (1871).\"\n",
        "\n",
        "tokens = word_tokenize(doc)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "print('Stemmed:' ,stemmed)\n",
        "\n",
        "#  Lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#process the text using spaCy\n",
        "doc = nlp(doc)\n",
        "\n",
        "filtered_doc = [word for word in doc if word not in stopwords.words('english')]\n",
        "\n",
        "lemmatized = [token.lemma_ for token in filtered_doc]\n",
        "print('Lemmatized: ', lemmatized)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOaOmdah4xKj",
        "outputId": "82f0c2cd-c294-4f77-eea2-a97404ea188e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed: ['charl', 'lutwidg', 'dodgson', ',', 'better', 'known', 'by', 'hi', 'pen', 'name', 'lewi', 'carrol', ',', 'wa', 'an', 'english', 'author', ',', 'poet', ',', 'mathematician', 'and', 'photograph', '.', 'hi', 'most', 'notabl', 'work', 'are', 'alic', \"'s\", 'adventur', 'in', 'wonderland', '(', '1865', ')', 'and', 'it', 'sequel', 'through', 'the', 'looking-glass', '(', '1871', ')', '.']\n",
            "Lemmatized:  ['Charles', 'Lutwidge', 'Dodgson', ',', 'well', 'know', 'by', 'his', 'pen', 'name', 'Lewis', 'Carroll', ',', 'be', 'an', 'english', 'author', ',', 'poet', ',', 'mathematician', 'and', 'photographer', '.', 'his', 'most', 'notable', 'work', 'be', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', '(', '1865', ')', 'and', 'its', 'sequel', 'through', 'the', 'Looking', '-', 'Glass', '(', '1871', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS (Part Of Speech)\n",
        "\n",
        "Using the pos_tag function in NLTK, we can check the part of speech (the role that some word has in a sentence) of a certain token within the sentence:"
      ],
      "metadata": {
        "id": "H1eayxdP-CRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuYvso2c-Q2U",
        "outputId": "a3a5eacd-fdc3-4675-9957-e86969714032"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "tagged = pos_tag(tokens)\n",
        "for word, tag in tagged:\n",
        "    print(f'Word: {word}, POS: {tag}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJmVFmGY-B90",
        "outputId": "69bc2b47-d1f0-40b5-e2d2-21a55edfd7d7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: Charles, POS: NNP\n",
            "Word: Lutwidge, POS: NNP\n",
            "Word: Dodgson, POS: NNP\n",
            "Word: ,, POS: ,\n",
            "Word: better, POS: JJR\n",
            "Word: known, POS: VBN\n",
            "Word: by, POS: IN\n",
            "Word: his, POS: PRP$\n",
            "Word: pen, POS: JJ\n",
            "Word: name, POS: NN\n",
            "Word: Lewis, POS: NNP\n",
            "Word: Carroll, POS: NNP\n",
            "Word: ,, POS: ,\n",
            "Word: was, POS: VBD\n",
            "Word: an, POS: DT\n",
            "Word: English, POS: JJ\n",
            "Word: author, POS: NN\n",
            "Word: ,, POS: ,\n",
            "Word: poet, POS: NN\n",
            "Word: ,, POS: ,\n",
            "Word: mathematician, POS: JJ\n",
            "Word: and, POS: CC\n",
            "Word: photographer, POS: NN\n",
            "Word: ., POS: .\n",
            "Word: His, POS: PRP$\n",
            "Word: most, POS: RBS\n",
            "Word: notable, POS: JJ\n",
            "Word: works, POS: NNS\n",
            "Word: are, POS: VBP\n",
            "Word: Alice, POS: NNP\n",
            "Word: 's, POS: POS\n",
            "Word: Adventures, POS: NNS\n",
            "Word: in, POS: IN\n",
            "Word: Wonderland, POS: NNP\n",
            "Word: (, POS: (\n",
            "Word: 1865, POS: CD\n",
            "Word: ), POS: )\n",
            "Word: and, POS: CC\n",
            "Word: its, POS: PRP$\n",
            "Word: sequel, POS: NN\n",
            "Word: Through, POS: IN\n",
            "Word: the, POS: DT\n",
            "Word: Looking-Glass, POS: NNP\n",
            "Word: (, POS: (\n",
            "Word: 1871, POS: CD\n",
            "Word: ), POS: )\n",
            "Word: ., POS: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset('NN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5GXV2fW_PEQ",
        "outputId": "efd67d48-eb9a-4005-f7cb-e6730bb203a5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand what each tagging means"
      ],
      "metadata": {
        "id": "siPmatV8_TiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)\n",
        "\n",
        "This is a more advanced NLP task that involves identifying and classifying key information (entities) in the text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, etc. NER is typically performed after the basic preprocessing steps.\n",
        "\n",
        "While preprocessing is about preparing and cleaning the text, NER is about analyzing its content to extract meaningful information."
      ],
      "metadata": {
        "id": "-Sd_nyCH_huJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "wQ-iEjuAAJol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = 'Apple is looking at buying U.K. startup for $1 billion'\n",
        "tokens = word_tokenize(doc)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "entities = ne_chunk(tagged)\n",
        "print(entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNblb9xE_Yqa",
        "outputId": "294e649a-e3db-4dda-9f0e-c4e1c1f1461a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Apple/NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization\n",
        "\n",
        "Is the process to transform text data to numerical format(embeddings), enabling machines to understand and process language.\n",
        "\n",
        "\n",
        "These vectors of numbers are called “embeddings”, capturing semantic meanings, relationships, and various linguistic properties of words."
      ],
      "metadata": {
        "id": "27pJvZi0Fahu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#“Bag of Words” (BoW)\n",
        "Is a term used in NLP that refers to a way of extracting features from text for use in modeling, such as in machine learning algorithms.\n",
        "\n",
        "In the BoW model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity."
      ],
      "metadata": {
        "id": "jsXWkHwlGHcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "doc = 'Why, sometimes I`ve believed as many as 6 impossible things before breakfast. There goes the shawl again!'\n",
        "tokens = word_tokenize(doc)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "filtered_words = [word for word in tokens if not word.lower() in stopwords.words('english')]\n",
        "\n",
        "bow = Counter(filtered_words)\n",
        "print(bow)\n",
        "\n",
        "# To understand the concept of creating the vocabulary and frequency of the words, we use Counter from python collections.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TCLMsjmGlQd",
        "outputId": "67191352-99b5-4de4-9538-9b7579f398ae"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({',': 1, 'sometimes': 1, '`': 1, 'believed': 1, 'many': 1, '6': 1, 'impossible': 1, 'things': 1, 'breakfast': 1, '.': 1, 'goes': 1, 'shawl': 1, '!': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "docs = ['This is the first document.','This document is the second document.','And this is the third one.', 'This one is the 4th']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
        "print(\"Vectorized representation:\\n\", X.toarray())\n",
        "\n",
        "# CounterVectorizer\n",
        "# It creates a vocabulary of all the unique words in the corpus and represents each document as a vector with counts of how often each word appears.\n",
        "# Characteristic: It treats every word as equally important and does not account for the context in which a word appears. The order of words is also ignored."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzGcxsQfLk8c",
        "outputId": "d0576f9a-c856-450f-8366-bfd340321843"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['4th' 'and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "Vectorized representation:\n",
            " [[0 0 1 1 1 0 0 1 0 1]\n",
            " [0 0 2 0 1 0 1 1 0 1]\n",
            " [0 1 0 0 1 1 0 1 1 1]\n",
            " [1 0 0 0 1 1 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF (Term Frequency-Inverse Document Frequency):\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Load the Documents\n",
        "documents = [\n",
        "    \"I love reading books.\",\n",
        "    \"Reading in the morning is refreshing.\",\n",
        "    \"I love morning coffee.\"\n",
        "]\n",
        "\n",
        "# Step 2: Tokenize and Preprocess the Documents\n",
        "tokens = [word_tokenize(doc) for doc in documents]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "cleaned_docs = []\n",
        "for token_list in tokens:\n",
        "    cleaned_doc = [word.lower() for word in token_list if word.lower() not in stop_words]\n",
        "    cleaned_docs.append(' '.join(cleaned_doc))\n",
        "\n",
        "# Step 3: Apply TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(cleaned_docs)\n",
        "\n",
        "# Step 4: Analyze the Output\n",
        "# Print the vocabulary (unique words)\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the TF-IDF values for each document\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6iYdGGKL2dy",
        "outputId": "1bf814d7-3d6f-434b-b61b-6b66539f36a3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['books' 'coffee' 'love' 'morning' 'reading' 'refreshing']\n",
            "TF-IDF Matrix:\n",
            "[[0.68091856 0.         0.51785612 0.         0.51785612 0.        ]\n",
            " [0.         0.         0.         0.51785612 0.51785612 0.68091856]\n",
            " [0.         0.68091856 0.51785612 0.51785612 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TF-IDF Matrix is a 3x9 matrix where each row represents one of your documents and each column corresponds to one of the words in the vocabulary.\n",
        "The values in the matrix represent the TF-IDF score of each word in each document.\n",
        "A higher TF-IDF score indicates a word is more important or relevant in a specific document."
      ],
      "metadata": {
        "id": "F0wxJa-bPXc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "PgxUnIODRZGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "data =[{\"title\": \"The Road Not Taken\", \"author\": \"Robert Frost\", \"text\": \"Two roads diverged in a wood, and I\\u2014\\nI took the one less traveled by,\\nAnd that has made all the difference.\"},\n",
        "{\"title\": \"Still I Rise\", \"author\": \"Maya Angelou\", \"text\": \"You may write me down in history\\nWith your bitter, twisted lies,\\nYou may tread me in the very dirt\\nBut still, like dust, I'll rise.\"},\n",
        "{\"title\": \"If\", \"author\": \"Rudyard Kipling\", \"text\": \"If you can keep your head when all about you\\nAre losing theirs and blaming it on you;\\nYours is the Earth and everything that's in it,\\nAnd\\u2014which is more\\u2014you'll be a Man, my son!\"}]"
      ],
      "metadata": {
        "id": "PzmxCx1TR2j2"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#cleaning stopwords\n",
        "processed_poems = []\n",
        "for poem in data:\n",
        "    tokens = word_tokenize(poem['text'])\n",
        "    filtered_tokens = [word for word in tokens if not word.lower() in stopwords.words('english') and word.isalpha()]\n",
        "    processed_poems.append(' '.join(filtered_tokens))\n",
        "\n",
        "\n",
        "# Step 3: Apply TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(processed_poems)\n",
        "\n",
        "# Step 4: Analyze the Output\n",
        "# Print the vocabulary (unique words)\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the TF-IDF values for each document\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VOUqNsnRXfG",
        "outputId": "466e3775-9210-4abb-ef5f-177243878fec"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['bitter' 'blaming' 'difference' 'dirt' 'diverged' 'dust' 'earth'\n",
            " 'everything' 'head' 'history' 'keep' 'less' 'lies' 'like' 'losing' 'made'\n",
            " 'man' 'may' 'one' 'rise' 'roads' 'son' 'still' 'took' 'traveled' 'tread'\n",
            " 'twisted' 'two' 'wood' 'write']\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.31622777 0.         0.31622777 0.\n",
            "  0.         0.         0.         0.         0.         0.31622777\n",
            "  0.         0.         0.         0.31622777 0.         0.\n",
            "  0.31622777 0.         0.31622777 0.         0.         0.31622777\n",
            "  0.31622777 0.         0.         0.31622777 0.31622777 0.        ]\n",
            " [0.25819889 0.         0.         0.25819889 0.         0.25819889\n",
            "  0.         0.         0.         0.25819889 0.         0.\n",
            "  0.25819889 0.25819889 0.         0.         0.         0.51639778\n",
            "  0.         0.25819889 0.         0.         0.25819889 0.\n",
            "  0.         0.25819889 0.25819889 0.         0.         0.25819889]\n",
            " [0.         0.35355339 0.         0.         0.         0.\n",
            "  0.35355339 0.35355339 0.35355339 0.         0.35355339 0.\n",
            "  0.         0.         0.35355339 0.         0.35355339 0.\n",
            "  0.         0.         0.         0.35355339 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "\n",
        "Is a method of extracting features from text and creating vectors with those feautures.\n",
        "\n",
        "* Purpose: Generates word embeddings, i.e., it represents words in a dense vector space.\n",
        "* Mechanism: Uses neural networks to learn word associations from a large corpus of text. Each word is represented by a densely populated vector of several hundred dimensions.\n",
        "* Characteristic: Captures more complex word relationships and similarities based on context. Words that appear in similar contexts have similar embeddings.\n",
        "* Use Case: Ideal for tasks that require understanding word meanings and relationships, such as sentiment analysis and machine translation."
      ],
      "metadata": {
        "id": "ew1gJg_bSCgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nltk gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqV1V4O0SP7N",
        "outputId": "6e8aa1ea-c7b2-40d5-8e1e-ee865ec6876e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "nltk.download('punkt')\n",
        "\n",
        "dataset = api.load(\"text8\")  # Loading a sample text dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COpVQj27TIsF",
        "outputId": "fcd9b260-2a85-4586-b22b-2bb5a6773d43"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [' '.join(doc) for doc in dataset]\n",
        "tokenized_data = [word_tokenize(doc) for doc in dataset]"
      ],
      "metadata": {
        "id": "IAzShqUyUXhP"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=3, min_count=1, workers=6)"
      ],
      "metadata": {
        "id": "-KlKFDvsTM2x"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_result = model.wv.most_similar(positive = ['woman', 'king'], negative = ['man'], topn=1)\n",
        "print(example_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NLjlzdFTijw",
        "outputId": "b8e33d16-b788-48e3-94f0-ec5d28d4102e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.6338568329811096)]\n"
          ]
        }
      ]
    }
  ]
}